---
title: "ENHANCED ACOUSTIC TAGGING, ANALYSIS, AND REAL-TIME MONITORING OF WILD AND
  HATCHERY SALMONIDS IN THE SACRAMENTO RIVER VALLEY"
params:
  printcode: no
  begindate: "September 09, 2025"
  enddate: "September 30, 2025"
author: "Semi-annual report `r params$begindate` to `r params$enddate`"
output:
  word_document:
    reference_docx: template.docx
    toc: yes
    toc_depth: '1'
    fig_caption: yes
  html_document:
    toc: yes
    toc_depth: '1'
    df_print: paged
  pdf_document:
    toc: yes
    toc_depth: '1'
    fig_caption: yes
always_allow_html: yes
---

```{r setup, include=FALSE, class.source = 'fold-hide'}
knitr::opts_chunk$set(echo = params$printcode, message = FALSE)
knitr::opts_knit$set(root.dir = '~/Desktop/BORSemiAnnualReport/BORSemiAnn2026/')

# load data access packages
library(odbc) #Read in Access database off of SQL server
library(dataRetrieval)
library(rerddap)
library(dbplyr)
library(DBI) #R database management
#load data manip packages
library(tidyverse)
library(data.table)
library(grid)
library(fs)
library(lubridate)
#load report generation packages
library(shiny)
library(rmarkdown) #generate word report
library(knitr) #generate word report
library(flextable) #format tables
library(officer)
library(grid)
library(ggplot2)
library(plotly)

cache_delete_all()

#set border style first time
bord = fp_border(color="#9bb4df", width=1)

#store and format date params
begindatechar<- params$begindate
begindate<- lubridate::mdy(params$begindate)
beginmo<- str_extract(begindatechar, "^.*(?=[[:punct:]])")
enddatechar<- params$enddate
enddate<- mdy(params$enddate)
WY<- str_extract(params$enddate, "\\d{4}")
```

Prepared for:\
United States Bureau of Reclamation\
Cooperative Agreement R21AC10455

Principal Investigator:\
Cyril Michel\
University of California, Santa Cruz\
Associated with\
NOAA Southwest Fisheries Science Center\
Fisheries Ecology Division\
110 McAllister Way\
Santa Cruz CA 95060\
[cyril.michel\@noaa.gov](mailto:cyril.michel@noaa.gov){.email}

Technical Point of Contact:\
Jeremy Notch\
University of California, Santa Cruz\
Associated with\
NOAA Southwest Fisheries Science Center\
Fisheries Ecology Division\
110 McAllister Way\
Santa Cruz CA 95060\
[jeremy.notch\@noaa.gov](mailto:jeremy.notch@noaa.gov){.email}

\newpage

\tableofcontents

\newpage

# Introduction

This report summarizes the fieldwork, data collection, and analysis performed by UC Santa Cruz (UCSC) between `r begindatechar` and `r enddatechar`, as part of the Cooperative Agreement R25AC00575 between the US Bureau of Reclamation (USBR) and UCSC. This is the first semi-annual report for the Cooperative Agreement R25AC00575, extending from September 9, 2025 to September 30, 2028. This semi-annual report outlines deliverables for the six tasks described by the agreement.

```{r call-er, include=FALSE}
##Table 1. Location of real-time JSATS receivers with the date first operational in real-time

con <- dbConnect(odbc::odbc(),
                    Driver = "ODBC Driver 17 for SQL Server",
                    Server = "calfishtrack-server.database.windows.net",
                    Database = "JSATS_Database",
                    UID = "jsats_user",
                    PWD = "Pass@123",
                    Port = 1433)
odbcListObjects(con)

```

# Task 1. Deploy real-time array of JSATS receivers

```{r callrec, echo = FALSE}
##Table 1. Location of real-time JSATS receivers with the date first operational in real-time

#call receiver deployment table
Rec_Dep<-dbReadTable(con, "ReceiverDeployments")

#remove NA and 999 named receivers
Rec_Dep2<- Rec_Dep[!is.na(Rec_Dep$Location), ]
Rec_Dep2<- Rec_Dep2[Rec_Dep2$Location != "999", ]
Rec_Dep2<- Rec_Dep2[Rec_Dep2$SN != "1", ]

#create start date col
Rec_Dep2$StartDate<-as.Date(Rec_Dep2$StartTime,format = "%Y-%m-%d", optional=T)
#create end date col
Rec_Dep2$EndDate<-as.Date(Rec_Dep2$EndTime, format = "%Y-%m-%d", optional=T)

#Get deployments in range interested in
start<-Rec_Dep2[Rec_Dep2$StartDate >= begindate,]
#this is the end date for period of interest
start2<-start[start$StartDate <= enddate,]

#grab receivers ended in period of interest
end<- Rec_Dep2[Rec_Dep2$EndDate >= begindate,]
end2<-end[end$EndDate <= enddate,]

#datesdeployed<- write.csv(Rec_Dep2, "/Users/jessefrey/Desktop/2024deploys.csv")

# #grab receivers still active
# notend<- Rec_Dep2[is.na(Rec_Dep2$EndDate), ]
# notend<- notend[notend$StartDate >= begindate, ]

#Make sure GPS names match
Rec_Dep3<-unique(rbind(start2, end2))
#remove NA receiver name brought in from date filters and JSCS receivers
Rec_Dep3<- Rec_Dep3[!is.na(Rec_Dep3$Location), ]
#remove mcCloud receivers
drop<- c('^JSCS','^Temp_Curt','Keswick1','^WC_')
Rec_Dep3<- filter(Rec_Dep3,!grepl(paste(drop, collapse='|'), Location))
Rec_Dep3<- Rec_Dep3 %>% dplyr::rename("GPSname" = "Location")

#read in receiver locations table
Rec_Loc<-dbReadTable(con, "ReceiverLocations")
#remove 999 named location
Rec_Loc<- Rec_Loc[Rec_Loc$GPSname != "999", ]
Rec_Loc<- Rec_Loc[Rec_Loc$Region != "McCloud R", ]

#test Loc name matches
test<- left_join(Rec_Dep3, Rec_Loc, by="GPSname")
notmatches<- as.data.frame(which(is.na(test$GeneralLocation), arr.ind=TRUE))

#show not matches
notmatches1 <- filter(Rec_Dep3, row_number() %in% notmatches$`which(is.na(test$GeneralLocation), arr.ind = TRUE)`)
#Meridian is spelled wrong
#Rec_Dep3$GPSname<- gsub("Meridan", "Meridian", Rec_Dep3$GPSname)

if(length(notmatches) > 1) { 
  stop("Deployment Location doesn't match General Location, see notmaches1")
}

# write.csv(notmatches1, "~/Desktop/BORSemiAnnualReport/BORSemiAnn2025Q1-2/notmatches1.csv")
```

```{r cov, echo=FALSE, warning=FALSE}
#Review coverage for descriptive summary
######################################################################################
#find sites with multiple deployments
dupes <- Rec_Dep3[duplicated(Rec_Dep3$GPSname),]
#select all multiple deployments
redeploys<- filter(Rec_Dep3, GPSname %in% dupes$GPSname)
redeploys<- redeploys %>% 
          arrange(GPSname, StartTime) %>% 
          summarise(GPSname, StartTime, DataCoverage, CoverageProblem,
                    RecDeployNotes)
######################################################################################

#select receiver location columns
Rec_Loc2<-subset(Rec_Loc, select = c("GeneralLocation","rkm", "GPSname", "Region", "Lat", "Lon"))
#join receiver deployments and receiver locations 
Rec_Dep4<-unique(merge(Rec_Dep3,Rec_Loc2, all.x=TRUE))
#reorder columns
Rec_Dep5<-Rec_Dep4[,c("Region","GPSname","Lat","Lon","rkm","RecMake","SN","StartTime","EndTime")]
#remove comma from SN
Rec_Dep5$SN<- as.character(Rec_Dep5$SN)
#remove row numbers
rownames(Rec_Dep5) <- c()

#Select real-time receivers
Rec_Dep6<-Rec_Dep5[which(Rec_Dep5$RecMake=="ATS SR3017" | Rec_Dep5$RecMake=="ATS 3017" |
                           Rec_Dep5$RecMake=="Tekno RT" | Rec_Dep5$RecMake=="Tekno" |
                           is.na(Rec_Dep5$RecMake)),]

#Order table by StartTime
Rec_Dep6<- Rec_Dep6[order(as.Date(Rec_Dep6$StartTime, format="%d-%m-%Y")),]

#Count real-time receivers by Region
regs0<- dplyr::count(unique(Rec_Dep6),Region)

#### may need to be updated
sac<- regs0 %>% filter(str_detect(regs0$Region, "^.*Sac"))
sac<- sac$n
delta <- regs0 %>% filter(str_detect(regs0$Region, "^.*Delta"))
delta<- delta$n
gg <- regs0 %>% filter(str_detect(regs0$Region, "SF Bay"))
gg<- gg$n
cs<- regs0 %>% filter(str_detect(regs0$Region, "^Carq"))
cs<- cs$n
sj<- regs0 %>% filter(str_detect(regs0$Region, "San Joaquin River"))

sj<- sj$n
sr<- regs0 %>% filter(str_detect(regs0$Region, "Stanislaus River"))
sr<- sr$n
```

There were no real-time receivers deployed and/or retrieved during this report period.

```{r table1, , eval=FALSE, echo=FALSE}

#pipe data to table and give caption
RecDep6<- Rec_Dep6%>%
  flextable() %>%
    set_caption(caption = "Table 1. Location of real-time JSATS receivers deployed during this report period with the date first operational in real-time")%>%
    font(fontname = "Times New Roman", part = "all") %>% 
    fontsize(size = 11, part = "body")%>%
    width(width = .75)

#set table style
RecDep6<- theme_zebra(RecDep6, odd_header="#4472c4", odd_body ="#d9e2f3", even_header="transparent", even_body="transparent")
RecDep6<- border_outer(RecDep6, part="all", border= bord)
RecDep6<- border_inner(RecDep6, part="body", border=bord)
RecDep6<- align(RecDep6, align = "center", part = "all")


#print table
#RecDep6

```

Deliverables 1-4 were met by posting data relating to survival and movement to the website: <https://oceanview.pfeg.noaa.gov/CalFishTrack>

## Deliverables

1.  Web-accessible reporting status of real-time receivers
2.  Real-time data available through ERDDAP data server, updated daily
3.  Web-accessible real-time receiver data available in open data format
4.  Web-accessible summary database of deployment of receivers
5.  Data quality assurance of no more than 3 days of downtime before a site visit to reestablish real-time operations
6.  Provide a semi-annual compiled raw data file for each real-time receiver along with deployment metadata to Arnold Ammann (NMFS)

# Task 2. Deploy autonomous array of JSATS receivers

## Autonomous Receiver Deployment

```{r callat, echo=FALSE}
##Table 2. Autonomous JSATS receivers active, deployed, or retrieved between April 1, 2022, and September 30, 2023

#Collect autonomous JSATS
#Select real-time receivers
Rec_Dep7<-Rec_Dep5[which(Rec_Dep5$RecMake!="ATS SR3017" & 
                               Rec_Dep5$RecMake!="ATS 3017" &
                               Rec_Dep5$RecMake!="Tekno RT" &
                               Rec_Dep5$RecMake!="Tekno"),]

#Count receivers by Region
regs1<- dplyr::count(Rec_Dep7,Region)

#### may need to be updated
sac1<- regs1 %>% filter(str_detect(regs1$Region, "^.*Sac"))
sac1<- sac1$n
delta1 <- regs1 %>% filter(str_detect(regs1$Region, "^.*Delta"))
delta1<- delta1$n
gg1 <- regs1 %>% filter(str_detect(regs1$Region, "SF Bay"))
gg1<- gg1$n
sj1<- regs1 %>% filter(str_detect(regs1$Region, "San Joaquin River"))
sj1<- sj1$n
sr1<- regs1 %>% filter(str_detect(regs1$Region, "San Joaquin River"))
sr1<- sr1$n
```

There were no autonomous receivers retrieved and/or deployed during this report period.

```{r table2, eval=FALSE, echo=FALSE}

#Order table by StartTime
Rec_Dep7<-Rec_Dep7[order(as.Date(Rec_Dep7$StartTime, format="%d-%m-%Y")),]
#remove row numbers
rownames(Rec_Dep7) <- c()
#format SNs
Rec_Dep7$SN<- as.character(Rec_Dep7$SN)

#pipe data to table and give caption
RecDep7<- Rec_Dep7 %>%
    flextable() %>%
  set_caption("Table 2. Autonomous JSATS receivers deployed and retrieved during this report period") %>%
  font(fontname = "Times New Roman", part = "all") %>%
  fontsize(size = 11, part = "body") %>%
  width(width = .75)

#set table style
RecDep7<- theme_zebra(RecDep7, odd_header="#4472c4", odd_body ="#d9e2f3", even_header="transparent", even_body="transparent")
RecDep7<- border_outer(RecDep7, part="all", border= bord)
RecDep7<- border_inner(RecDep7, part="body", border=bord)
RecDep7<- align(RecDep7, align = "center", part = "all")

#print table
#RecDep7
```

## Deliverables:

1.  Provide data to ITAG JSATS Database coordinator and web-accessible autonomous receiver data (via ERDDAP) within 30 days of downloading
2.  Web-accessible semi-annual log of deployment and download activity including what sites were visited and operational coverage for each receiver

# Task 3. Implant AT into a portion of hatchery produced Chinook salmon juveniles

## Acoustic Tagging of Chinook Salmon juveniles

### Seasonal Survival Study

No Seasonal Survival study fish were tagged during this report period. 

As part of the 2025 WY Seasonal Survival Study, UCSC staff implanted acoustic tags into 800 hatchery produced jumpstart winter-run and late-fall run Chinook smolts as part of the Seasonal Survival Study. Releases of these fish were spaced out across four weeks between 12/09/2024 - 03/18/2025 with the intention of gathering movement and survival data across a range of environmental conditions during winter. These fish were tagged at Coleman, and Livingston Stone Fish Hatcheries and transported to the Red Bluff Diversion Dam (RBDD_Rel) to increase the sample size of fish in downstream reaches. Fish were available for tagging with the help and assistance of the U.S. Fish and Wildlife Service. 

The Tag Life Study for these releases, in progress as of the last report, is included in this report.

Preliminary movement and survival data can be found here: <https://oceanview.pfeg.noaa.gov/CalFishTrack/pageSeasSurv_2023.html>

```{r SSfish, echo=FALSE}
#Select studies to look at
Study<- paste("Seasonal_Survival", sep="_", WY)
tagss<-tag[which(tag$StudyID==Study),]

s2<-unique(tagss[,c("StudyID","Fish_Type","Raceway", "Rel_loc","DateTagged","Rel_datetime")])

tagss$fish_number<-1

#Create summary table
SStag_table<- plyr::ddply(tagss, .(Raceway, DateTagged), summarise,count=sum(fish_number), 
                 meanW=mean(Weight), meanL=mean(Length), 
                 SD_W=sd(Weight), SD_L=sd(Length), min_W=min(Weight), min_L=min(Length),
                 max_W=max(Weight), max_L=max(Length))

#Set decimal place to 2
SStag4<-as.data.frame(lapply(SStag_table[sapply(SStag_table, is.numeric)], round, 2))
#Add back in rel datetime row
SStag4$DateTagged<-SStag_table$DateTagged

SStag5<-merge(SStag4,s2)

SStag5<-SStag5[order(SStag5$Rel_datetime),]

#Reduced table for report
SStag6<-SStag5[,c("StudyID", "Fish_Type", "Rel_loc", "Raceway", "count", "Rel_datetime", "meanW", "meanL")]
names(SStag6)[names(SStag6)=="Rel_loc"]<-"Release Loc"
names(SStag6)[names(SStag6)=="count"]<-"Count"
names(SStag6)[names(SStag6)=="Rel_datetime"]<-"Release Date"
names(SStag6)[names(SStag6)=="meanW"]<-"Avg Weight"
names(SStag6)[names(SStag6)=="meanL"]<-"Avg Length"
names(SStag6)[names(SStag6)=="Fish_Type"]<-"Fish Type"

#sstotal<- sum(SStag6$Count)

```

```{r SStable, echo=FALSE}
#pipe data to table and give caption
SStag6<- SStag6%>%
  flextable() %>%
    set_caption(caption = "Table 4. Hatchery fish acoustic tagged during this report period.") %>%
    font(fontname = "Times New Roman", part = "all") %>%
    fontsize(size = 11, part = "body") %>% 
    width(width = .75)

#set table style
SStag6<- theme_zebra(SStag6, odd_header="#4472c4", odd_body ="#d9e2f3", even_header="transparent", even_body="transparent")
SStag6<- border_outer(SStag6, part="all", border= bord)
SStag6<- border_inner(SStag6, part="body", border=bord)
SStag6<- align(SStag6, align = "center", part = "all")

#print table
SStag6

```

### Spring Pulse Flow Study

No Pulse Flow study fish were tagged during this report period.

## Tag Retention Studies
```{r trtags, echo=FALSE, include=FALSE, eval=FALSE}
##format pit tags
options(scipen = 999)
# #read in data from date tagged
start<- dbReadTable(con, "Tag_Effects_Start")
# #read in data from date dissection
end<- dbReadTable(con, "Tag_Effects_End")
# #read in tag shed date
# shed<- read.csv("~/Desktop/BORSemiAnnualReportQ3Q4DESK/Tag_Effects/Tag_Effects_Shed_Mort_List.csv", header = T)


#format colnames
start2<- start %>% 
  subset(select=c("TagID_Hex","StudyID","DateTagged","Weight_Start","Length_Start")) 
end2<- end %>% 
  subset(select= c("TagID_Hex", "StudyID","Date_Euthanized", "Weight_End",
                   "Length_End","Tag.Shed._Y.N", "Date_Mort", "Mortality_Y.N")) %>%
  na.omit(weight_end)

# shed2<- shed %>% 
#   dplyr::rename(c("weight_end" = "Weight_recovery", "length_end" = "Length_recovery", "Tag.ID" = "TagID_Hex"))

#join by study and acoustic tag ID (tags are reused)
tagRTfish<- merge(start2, end2, by=c('TagID_Hex', 'StudyID'), all.x=TRUE)

```

```{r sstr, echo = FALSE, eval=FALSE}
#summarise seasonal survival
Study<- paste("Seasonal_Survival", sep="_", WY)

SStagRT<- tagRTfish[which(tagRTfish$StudyID==Study),] 
SStagRT<- SStagRT %>%
  mutate(as.Date(DateTagged),
         as.Date(Date_Mort))
# count tagged
sstotal<- nrow(SStagRT)
# count shed
SSshed<- dplyr::count(SStagRT[which(SStagRT$Tag.Shed._Y.N=="Y"),])
#count morts
SSmorts<- dplyr::count(SStagRT[which(SStagRT$Mortality_Y.N=="Y"),])
#remove morts for stats
SStagRT2<- SStagRT %>% filter(is.na(Mortality_Y.N))
  #average weight gain
  SSg<- signif(mean(SStagRT2$Weight_End-SStagRT2$Weight_Start), 3)
  #average length growth
  SSl<- signif(mean(SStagRT2$Length_End-SStagRT2$Length_Start), 3)

  # time to shed would require another table/csv with shed date
# minShed<- difftime(SStagRT$DateTagged - xxx)
# maxShed<-
  
# time to mort 
SStagRT<- SStagRT %>% mutate(
  mortT = difftime(SStagRT$Date_Mort, SStagRT$DateTagged, units = "days"))
firstmort<- SStagRT %>% slice(which.min(mortT))
minM<- firstmort$morT
minM<-as.numeric(minM)
lastmort<- SStagRT %>% slice(which.max(mortT))
maxM<-lastmort$mortT

#Summary table of studies and release dates
sstr<-unique(SStagRT[,c("StudyID","DateTagged")])
tag_str<- min(s0$DateTagged) # units dont match
tag_etr<- max(s0$DateTagged) # units dont match
tagtrt<- difftime(tag_e, tag_s, units= "weeks") # returns empty, units don't match 
tag_str<-format(as.Date(tag_s), '%m/%d/%Y')
tag_etr<-format(as.Date(tag_e), '%m/%d/%Y')

```

## Seasonal Survival Tag Retention Study

No Seasonal Survival study fish were tagged during this report period.


## Spring Pulse Tag Retention Study

No Pulse Flow study fish were tagged during this report period.

## Seasonal Survival Tag Life Study

```{r ttab, eval=FALSE, warning=FALSE, include=FALSE}
 
 taglife<- read.csv("~/Desktop/BORSemiAnnualReport/BORSemiAnn2025Q1-2/Tag_Life/Tag_Batterly_Life.csv", header = FALSE)
taglife<- taglife[-1,]
colnames(taglife) <- taglife[1,]
taglife<- taglife[-1,]
# fix scientifc notation
taglife$TagID_Hex<- gsub(".00", "", taglife$TagID_Hex)
taglife$TagID_Hex<- gsub("\\+", "", taglife$TagID_Hex)

#count tags started by study ID:
tagcount<- length(taglife$TagID_Hex)

#Summary table of start dates
starts<- taglife %>%
    count(Date) 
w1<- starts[1,1]
w2<- starts[2,1]
w3<- starts[3,1]
w4<- starts[4,1]

# ###Read in tag start data for all studies
# 
# # list the folders in this file
# files <- list.files("~/Desktop/BORSemiAnnualReportQ3Q4DESK/Tag_Life/StartUpSpecSheets/")
# 
# #Number of columns and column names must match exactly for this to work
# data2<-lapply(files, read.table, header=TRUE, sep=",")
# for (i in 1:length(data2)){data2[[i]]<-cbind(data2[[i]],files[i])}
# startup <- do.call("rbind", data2) 
# 
# startup2<-startup %>%
#   dplyr::rename(studyid = `files[i]`) %>%
#   mutate(Date = as.Date(DateTime, format = "%m/%d/%Y"),
#          studyid = sub("\\..*", "", studyid),
#          PRI_r = round(PRI, digits=0)) %>%
#   select(-c(Notes, PRI))
# 
# #Create warranty life table with all the options for tag model and PRI
# TagModel<-c("SS300","SS400","SS300 bat 392", "SS400 2 bat 379","SS300","SS400","SS300 bat 392", "SS400 2 bat 379","SS400","SS300","SS400","SS300 bat 392", "SS400 2 bat 379")
# PRI_r<-c(3,3,3,3,5,5,5,5,8,10,10,10,10)
# warranty_life<-c(23, 48, 79, 108, 37, 71, 128, 159, 90, 68, 111, 238, 247)
# warranty<-data.frame(TagModel, PRI_r, warranty_life)
# 
# #Add warranty life and warranty date to startup table
# startup3<-left_join(startup2, warranty) %>%
#   mutate(warranty_date = Date + warranty_life) %>%
#   dplyr::rename(start_date = Date)
# 
# #write.csv(startup3, "~/Desktop/BORSemiAnnualReportQ3Q4DESK/Tag_Life/Tag_Lifestartup.csv")
```

```{r readfiltered_detects, eval=FALSE, warning=FALSE, include=FALSE}
# #Set your wd to the file containing the 5 PRI filtered files
# setwd("~/Desktop/BORSemiAnnualReportQ3Q4DESK/Tag_Life/PRI_5_FilterFiles/")
# #Read in 5 or 3 PRI files (Only read in the TagCode and DateTime columns)
# Filt_Files_5PRI <- 
#   list.files(pattern = "*.csv") %>% 
#   map_df(~fread(.,select = c("TagCode","DateTime_PST","recv"))) %>%
#   mutate(PRI_r = 5)
# 
# #Set your wd to the file containing the 8 PRI filtered files (for the CDFW tags)
# setwd("~/Desktop/BORSemiAnnualReportQ3Q4DESK/Tag_Life/PRI_8_FilterFiles/")
# Filt_Files_8PRI <-
#   list.files(pattern = "*.csv") %>% 
#   map_df(~fread(.,select = c("TagCode","DateTime_PST", "recv"))) %>%
#   mutate(PRI_r = 8)
# 
# Filt_Files<-rbind(Filt_Files_5PRI, Filt_Files_8PRI)
# 
# #Some tags are missing a leading 0 and all have a space in front of them, remove the space and add the leading zero
# #Only do this for the Lotek detections, not the RT receiver detections (once these are included...)
# 
# Filt_Files2<-Filt_Files %>%
#   mutate(TagCode = ifelse(recv == 19030, TagCode, substr(TagCode, 3, 6)),
#          TagCode = ifelse(substr(TagCode, 1,1)==" ", paste("0", substr(TagCode, 2, 5), sep = ""), TagCode))
```

```{r daysdetected, eval=FALSE, include=FALSE}
# ###Get date dead for each tag by selecting max day a tag was detected on two consecutive days, with at least 100 detections on each day
# #Only do this for autonomous receiver files, not RT files
# 
# # rem<-Filt_Files2 %>%
# #   group_by(recv, PRI_r, TagCode) %>%
# #   dplyr::count(TagCode, recv, as.Date(DateTime_PST)) %>%
# #   mutate(diff = NA,
# #          diff = as.numeric(diff),
# #          n = as.numeric(n)) %>%
# #  dplyr::rename(Date2 = `as.Date(DateTime_PST)`,
# #          TagID_Hex = TagCode) %>%
# #   filter(Date2 > "2022-01-01") %>% #There were some weird detections in 2012, remove these
# #   mutate(remove = ifelse(recv!=19030 & n < 100, "Y",
# #                   ifelse(recv==19030 & n < 20, "Y", "N"))) %>% 
# #   filter(remove == "N") %>%
# #   arrange(recv, TagID_Hex, PRI_r, Date2)
# # 
# # 
# # #For each tag code, get the difference between date and previous day detected
# # for(i in 2:nrow(rem)) {
# #   if(rem[i, 'recv'] == rem[i - 1, 'recv'] & rem[i, 'TagID_Hex'] == rem[i - 1, 'TagID_Hex']) {
# #     rem[i, 'diff'] <- as.numeric(difftime(rem$Date2[i],rem$Date2[i - 1], units = "day"))
# #   }
# # }
# 
# 
# #When getting date dead for each tag, take the maximum day that a tag was detected on two consecutive days with at least 100 detections on both days
# date_dead<- rem %>%
#  dplyr::rename(date_dead = Date2) %>%
#   group_by(TagID_Hex, PRI_r) %>%
#   summarize(date_dead = max(date_dead))
# 
# 
# #Add date dead and days on to the startup dataframe
# startup4<-left_join(startup3, date_dead) %>%
#   mutate(days_on = as.numeric(difftime(date_dead, start_date)))
# 
# 
# #Add startup data to the detection file, remove tagIDs that aren't in the startup file
# rem2<-left_join(rem, startup4) %>%
#  dplyr::rename(Date = Date2) %>%
#   filter(!is.na(studyid)) %>%
#   mutate(detect = "Y") %>%
#   select(TagID_Hex, PRI_r, Date, studyid, start_date, detect, diff, n, TagModel) %>%
#   ungroup()
# 
# 
# 
# ###Next find and expand date ranges where tags were not detected
# 
# #Find date ranges where tags were not detected
# no_det<-rem2 %>%
#   mutate(diff = ifelse(is.na(diff), 0, diff)) %>%
#   filter(diff > 1) %>%
#   mutate(first = Date - 1,
#          last = Date - (diff-1))

# 
# #Expand on those date ranges
# no_det2<-setDT(no_det)[ , list(recv = recv, TagID_Hex = TagID_Hex, PRI_r = PRI_r, day = seq(last, first, by = "day")), by = 1:nrow(no_det)]
# 
# no_det3<-no_det2 %>%
#   mutate(detect = "N") %>%
#  dplyr::rename(Date = day) %>%
#   select(TagID_Hex, PRI_r, Date, detect, recv)
# 
# no_det4<-left_join(no_det3, startup4) %>%
#   mutate(diff = as.numeric(NA),
#          n = as.numeric(NA)) %>%
#   select(TagID_Hex, PRI_r, Date, studyid, start_date, detect, diff, n, TagModel, recv) %>%
#   ungroup()
# 
# rem3<-rbind(rem2, no_det4) %>%
#   mutate(days_on = as.numeric(difftime(Date, start_date), units = "days")) %>%
#   filter(days_on > 0) %>%
#   group_by(studyid) %>%
#   arrange(TagID_Hex)

  
#If there were no days without detections, need to run the line with only black as the specified color

# running line with only "black" produces error
# Error in `palette()`:
# ! Insufficient values in manual scale. 2 needed but only 1 provided.
# Backtrace:
#   1. base (local) `<fn>`(x)
#   2. ggplot2:::print.ggplot(x)
#   4. ggplot2:::ggplot_build.ggplot(x)
#   5. base::lapply(data, scales_map_df, scales = npscales)
#   6. ggplot2 (local) FUN(X[[i]], ...)
#      ...
#  13. ggplot2 (local) FUN(X[[i]], ...)
#  14. self$map(df[[j]])
#  15. ggplot2 (local) map(..., self = self)
#  16. self$palette(n)
#  17. ggplot2 (local) palette(...)

# rem3 %>% filter(PRI_r == 5) %>%
# ggplot(aes(days_on, TagID_Hex, group = TagID_Hex, color = detect)) +
#   geom_line() +
#   scale_color_manual(values = c("white", "black")) +
#   #scale_color_manual(values = "black") +
#   ylab("Tag ID (Hex)") + xlab("Days On") +
#   theme_bw() + theme(plot.title = element_text(hjust = 0.5), legend.position = "none",
#                      panel.grid.major=element_blank(), panel.grid.minor=element_blank())
# 
# 
# 
# ###Create plot that shows percent tags remaining by studyid (currently plotting all studies, can filter for specific studies)
# unique(startup4$studyid)
# #Add release week to the startup table
# startup5<-startup4 %>%
#   mutate(across(studyid, str_replace, "Butte_2023_Tag_Life", "Butte_Creek_2023"),
#          across(studyid, str_replace, "CDFW_BattLife_20230601", "CDFW"),
#          across(studyid, str_replace, "Seasonal_Survival_Wk1_4", "Seasonal_Survival"),
#          across(studyid, str_replace, "SJ_Wild_all", "SJ_Steelhead"),
#          across(studyid, str_replace, "Spring_Pulse_Wk1_5", "Spring_Pulse")) %>%
#   group_by(studyid) %>%
#   arrange(studyid, start_date, .by_group = T) %>%
#   mutate(NameID = match(start_date, unique(start_date))) %>%
#   mutate(week = paste(studyid, NameID, sep = "-"))

```

```{r SSTLstudy, eval=FALSE, include=FALSE}
##Get number of tags by study
# tags_by_study<-startup5 %>%
#   group_by(studyid) %>%
#   filter(!is.na(date_dead)) %>%
#   summarise(count = n_distinct(TagID_Hex), max_day = max(days_on), start_date_min = min(start_date), start_day_max = max(start_date))
# 
# 
# # produces warning: 
# # Error in `[.data.table`(setDT(tags_by_study), , list(week = week, days_on = seq(0,  : 
# # All items in j=list(...) should be atomic vectors or lists. If you are trying something like j=list(.SD,newcol=mean(colA)) then use := by group instead (much quicker), or cbind or merge afterwards.
# 
# dates_expstudy<-setDT(tags_by_study)[ , list(studyid = studyid, days_on = seq(0, max_day)), by = 1:nrow(tags_by_study)]
# 
# # end warn
# 
# #Create column for number of tags dead by study
# deadbystudy<-left_join(dates_expstudy, startup5) %>%
#   mutate(dead = ifelse(is.na(TagID_Hex), 0, 1)) %>%
#   group_by(studyid) %>%
#   arrange(days_on, .by_group = TRUE) %>%
#   mutate(cum_count = cumsum(dead))
# 
# percent_tags_bystudy<-left_join(startup6, deadbystudy) %>%
#   mutate(dead_percent = (cum_count/count)*100) %>%
#   mutate(percent_alive = 100 - dead_percent) %>%
#   mutate(studyid = sub("-.*", "", week)) %>%
#   select(c(TagID_Hex, studyid, week, dead_percent, percent_alive, start_date, date_dead, days_on, max_day, TagModel))

```


To monitor the battery life of the tags used for the Seasonal Survival Study, a 5% random sample was taken from the total proportion of tags used for each release group. In total, `r tagcount` SS400 tags were started over a period of 4 weeks (on `r w1`, `r w2`,`r w3`, and `r w4`) and placed in the tag life tank located at the NMFS-SWFSC lab for monitoring. Data collected in this study examined the range of battery life for these particular tags, in order to correct any discrepancies in survival estimates as a result of tags shutting off prematurely. #r n# tags in the `r WY` Seasonal Survival tag life study made it to the warranty life of 71 days. The average run time was #r avgrun# days with a range of #r mi# to #r ma# days.

```{r SSTLweek, eval=FALSE, include=FALSE}
##Get number of tags by study week
# tags_by_studyWK<-startup5 %>%
#   group_by(week) %>%
#   filter(!is.na(date_dead)) %>%
#   summarise(count = n_distinct(TagID_Hex), max_day = max(days_on), start_date_min = min(start_date), start_day_max = max(start_date))
# 
# dates_exp<-setDT(tags_by_studyWK)[ , list(week = week, days_on = seq(0, max_day)), by = 1:nrow(tags_by_studyWK)]
# 
# #Create column for number of tags dead by study week
# startup6<-left_join(dates_exp, startup5) %>%
#   mutate(dead = ifelse(is.na(TagID_Hex), 0, 1)) %>%
#   group_by(week) %>%
#   arrange(days_on, .by_group = TRUE) %>%
#   mutate(cum_count = cumsum(dead))
# 
# percent_tags<-left_join(startup6, tags_by_study) %>%
#   mutate(dead_percent = (cum_count/count)*100) %>%
#   mutate(percent_alive = 100 - dead_percent) %>%
#   mutate(studyid = sub("-.*", "", week)) %>%
#   select(c(TagID_Hex, studyid, week, dead_percent, percent_alive, start_date, date_dead, days_on, max_day, TagModel))

```

###Tags started week 1:
`r w1` A total of 10 acoustic tags (model SS400) were randomly selected to be used in this tag life study. Tags were started on `r w1` and placed into the tag life tank for the duration of the study. All tags in the RBDD Week 1 tag life study made it to the warranty life of 71 days and were detected consistently. 

###Tags started week 2:
`r w2` A total of 10 acoustic tags (model SS400) were randomly selected to be used in this tag life study. Tags were started on `r w2` and placed into the tag life tank for the duration of the study. All tags in the RBDD Week 1 tag life study made it to the warranty life of 71 days and were detected consistently. 

###Tags started week 3:
`r w3` A total of 10 acoustic tags (model SS400) were randomly selected to be used in this tag life study. Tags were started on `r w3`2 and placed into the tag life tank for the duration of the study. All tags in the RBDD Week 1 tag life study made it to the warranty life of 71 days and were detected consistently. 

###Tags started week 4:
`r w14` A total of 10 acoustic tags (model SS400) were randomly selected to be used in this tag life study. Tags were started on `r w14` and placed into the tag life tank for the duration of the study. All tags in the RBDD Week 1 tag life study made it to the warranty life of 71 days and were detected consistently. 

## Deliverables:

1.  Final Pre-season tagging plan available via the website
2.  Web-accessible Telemetry Study Summary no more than 96 hours after the release of fish
3.  Annual technical report summarizing results from the previous study year, including
tag life tests and tag effect tests, available via website
4.  Final report summarizing final results of study years 1 and 2, and preliminary results
of study year 3, available via website


# Task 4. Produce and deliver real-time metrics

The project website was updated with new web pages describing unique tagging studies, including release metadata, travel time, number of fish detected at each real-time receiver, and detection efficiency for dual-line receiver locations (Sacramento, Benicia). <https://oceanview.pfeg.noaa.gov/CalFishTrack/>. Tagging data were updated two days after fish were tagged. Data from real-time receivers was automatically updated every hour.

## Deliverables:

1.  Website daily updates of arrival times, movement rates, and percent detected for each release group beginning immediately after the release of the first group
2.  Website updated weekly with real-time data, summary statistics of real-time survival and routing, and predictions based on models fitted to historical late-fall Chinook data

# Task 5. Project Management

Bi-weekly CVEAT conference calls and monthly ITAG virtual meetings were scheduled and moderated by ITAG facilitator Flora Cordoleani of UC Santa Cruz during the reporting period. These CVEAT calls facilitate close coordination on tagging events and receiver deployments between the many field operation leaders for the many different telemetry projects. Monthly ITAG meetings are for higher-level coordination and long-term planning for the Central Valley telemetry programs, and is attended by both field operation leaders as well as higher level agency representatives.

## Deliverables:

1.  Semi-annual progress reports
2. UC Davis 69kHz Telemetry Database Coordinator will participate in the ITAG meetings and appropriate subgroup meetings
3. UC Davis 69kHz Telemetry Database Coordinator will work with agencies and stakeholders to address key data management questions
4. The ITAG facilitator will schedule meetings and take meeting notes, and make meeting notes accessible to public via an online platform
5. The ITAG facilitator will collect pre- and post-study summary forms from researchers and host them on the CalFishTrack website
6. The ITAG facilitator will provide a summary report of lTAG activities within 6 months of the completion of the last ITAG tagging effort for the water year
7. At least one Friday lunch time presentation to the BOR Bay Delta Office, to be coordinated by BOR
8. Present findings at the Sacramento River Science Partnership, Sacramento River Real-time Operations Group, and Peer-Review Process Group when requested
9. Present recent findings as either a presentation or poster at one regional conference per year (such as IEP and Bay Delta Science Conference)